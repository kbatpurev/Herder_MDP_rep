
---
title: "Proof of concept - Exploring human dimensions of market-based incentive design with reinfocement
  learning"
output:
  html_document: default
  pdf_document: default
date: "2025-08-07"
---

### Markov Decision Processes to predict ecosystem condition in Mongolia under different herding practices

*Markov Decision Processes (MDPs) is a probabilistic framework that computes the optimal course of action in a repeated-decisions scenario through iterative agent based learning. It belongs to a family of machine learning algorithms called Reinforcement Learning. Here we apply MDPs to explore how herder behaviour and practices affect ecosystem condition in Mongolian rangelands. We approach this problem as a spatially explicit multi-agent reinforcement learning problem (MARL) with herders as agents, goat rearing as actions, and environmental conditions as states and climate as a stochastic variable that impacts both actions and states.*


#### Github repository

###### https://github.com/kbatpurev/Herder_MDP_rep.git

##### Document structure 

###### (V1. Toy version with two agents with softmax policy)

1. Model 1 - Action based design
2. Model 2 - Outcome based design
3. Null model - No incentive 

======================================================================================================================
```{r setup, include=FALSE}
.libPaths()
#.libPaths(new="C:/Data/RStudio")
.libPaths(new="C:/rlibs/4.3.0/")
#.libPaths(new="C:/Data/RStudio")


library(jsonlite)
library(highr)
library(tidyverse)
library(dplyr)
library(quantmod) # for findPeaks fxn
library(MDPtoolbox)

# plotting libraries
library(corrplot)
library(RColorBrewer)
library(ggthemes)
library(ggpubr)
library(ggplot2)
library(Cairo)
library(extrafont)
extrafont::loadfonts()
library(kableExtra)
library(truncnorm)
```

### **I. Action based PES**

Action based PES is when incentives are rewarded for actions that land owners or herders take to reduce land use pressures on ecosystem, in this case reducing or maintaining the goat herd size to improve and or maintain rangeland condition. We define 4 different actions: choosing to have herd sizes a)less than 100,b) 200-400, c)400-600 and d)600 and above. Each of these actions bring rewards or income to varying degrees. The rangeland can be in one of four states: intact, good, poor or degraded. And the weather conditions are drawn from a probability distribution where a given year has 50% of being in average condition, 30% chance of being in drought condition and 20% chance of being rainy (higher than average rainfall). Here is a conceptual model of the action based design that we explore in this chapter. In this model, the PES incentives are described with green numbers associated with the 4 actions. Whilst the direct cash profit associated with each action is described in brown. 

```{r conceptual model abpes,echo=FALSE,include=TRUE}
knitr::include_graphics("action_based_pes_27Aug.jpg")

```

##### Problem formulation
Here we create a 5 x 5 raster where two agents herd goats in the same space.

```{r problem formulation abpes}
land_grid_x <- 5
land_grid_y <- 5
all_cells <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1))

# Actions (stocking rates)
actions <- c("<100", "200-400", "400-600", ">600")
action_income <- c("<100" = 5, "200-400" = 5, "400-600" = 5, ">600" = 3)

# Rangeland states
r_states <- c("intact", "good", "poor", "degraded")
r_code <- setNames(0:3, r_states)

# Weather probabilities
weather_probs <- c("drought" = 0.3, "rainy" = 0.2, "normal" = 0.5)

#Weather, transition and reward functions
#---------------------------------------------------------------------------
draw_weather <- function() {
  sample(names(weather_probs), size=1, prob = weather_probs)
}
```

##### Transition matrix
This is a long if-then statement that defines how rangeland states relate to each action and weather condition combination. This statements illustrates  of how we perceive actions and climatic conditions impact the rangeland states. 
```{r transition matrix abpes ,echo=TRUE}
transition_rangeland <- function(state, action, weather) {
  if (state == "degraded") return("degraded")  # terminal state
  
  probs <- NULL
  
  # Define probabilities based on action and weather
  if (action == ">600") {
    if (weather == "drought") {
      probs <- c("degraded" = 0.9, "poor" = 0.1)
    } else if (weather == "normal") {
      probs <- c("degraded" = 0.7, "poor" = 0.3)
    } else {  # rainy
      probs <- c("poor" = 0.8, "good" = 0.2)
    }
    
  } else if (action == "400-600") {
    if (weather == "drought") {
      probs <- c("poor" = 0.6, "good" = 0.4)
    } else if (weather == "normal") {
      probs <- c("good" = 0.7, "intact" = 0.3)
    } else {
      probs <- c("good" = 0.4, "intact" = 0.6)
    }
    
  } else if (action == "200-400") {
    if (weather == "drought") {
      probs <- c("good" = 0.3, "intact" = 0.7)
    } else if (weather == "normal") {
      probs <- c("good" = 0.1, "intact" = 0.9)
    } else {
      probs <- c("intact" = 1.0)
    }
    
  } else {  # action == "<100"
    probs <- c("intact" = 1.0)
  }
  
  return(sample(names(probs), 1, prob = probs))
}
```


##### 2. Agents, actions and rewards

##### Reward structure and policy
Here we define how income from actions (reward) are calculated. Reward is a function of how agents benefit from herd actions (in this case - herd size). The specific strategy an agent employs to weigh the pros and cons of each reward is called policy, in this case this strategy is a "softmax" approach which is to maximise rewards with a moderate tau (temperature) of 1. Temperature settings for the policy can vary between 2 and 0. 

###### *Lower tau → more greedy (e.g., 0.1)*
###### *Higher tau → more exploratory (e.g., 2.0)*
###### *tau = 1 is a moderate setting (default in many systems)*

```{r compute return abpes}
compute_reward <- function(action, same_cell, revisit,cell_condition) {
  r <- action_income[action]
  if (same_cell) r <- r * 0.7
  if (revisit) r <- r * 0.7
  
  #A rangeland condition multiplier in reward (with the assumption that herders would benefit from a good quality pasture)
  condition_modifier <- dplyr::case_when(
    cell_condition == "intact"   ~ 1.0,
    cell_condition == "good"     ~ 0.9,
    cell_condition == "poor"     ~ 0.7,
    cell_condition == "degraded" ~ 0.4,
    TRUE ~ 1.0
  )
  
  r <- r * condition_modifier
  return(r)
}

softmax <- function(x, tau = 1.0) {
  exp_x <- exp((x - max(x)) / tau)
  exp_x / sum(exp_x)
}
create_agent <- function(name, tau = 1.0) {
  list(
    name = name,
    location = sample_n(all_cells, 1),
    last_cell = NA,
    reward_memory = list(),
    memory_count = list(),
    tau = tau
  )
}
```

##### Action selection in space

Agent selects actions depending on where they are in the landscape in relation to other agents - this is to emulate the preference for non-overlapping grazing practices employed by herders, which only exists as a silent/social agreement and are often breached by outsiders and desperate herders. Agents also have to consider the last time they were in a cell, because they are penalised if they return to the same cell immediately - this is an attempt to emulate ecosystem recovery from land use. If an agent hasn't been to a cell before and never tried an action before then they assume reward to be an average of 2.This is clearly a naive and overly optimistic approach. Future simulation should focus on testing the sensitivity of this parameter.

```{r action selection abpes, echo=TRUE}
select_action <- function(agent) {
  candidates <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1), action = actions)
  candidates$reward <- apply(candidates, 1, function(row) {
    key <- paste(row[["x"]], row[["y"]], row[["action"]], sep = "_")
    if (!is.null(agent$reward_memory[[key]]) && agent$memory_count[[key]] > 0) {
      avg <- agent$reward_memory[[key]] / agent$memory_count[[key]]
    } else {
      avg <- 2 
    }
   
    # Safe revisit check
    same_as_last_cell <- isTRUE(agent$last_cell["x"] == as.numeric(row[["x"]]) &&
                                  agent$last_cell["y"] == as.numeric(row[["y"]]))
    if (same_as_last_cell) avg <- avg * 0.7
    return(avg)
  })
  
  probs <- softmax(candidates$reward, agent$tau)
  choice <- sample(1:nrow(candidates), 1, prob = probs)
  return(candidates[choice, ])
}
```

Alternative to using a constant of 2 as an average reward there is the option of using Bayesian smoothing based on prior knowledge through a small step like this: 

```{r alternative - Bayesian smoothing, eval=FALSE, include=FALSE}
#prior_mean <- 1.5
#prior_count <- 1
#numerator <- ifelse(!is.null(agent$reward_memory[[key]]), agent$reward_memory[[key]], 0) + prior_mean*prior_count
#denominator <- ifelse(!is.null(agent$memory_count[[key]]), agent$memory_count[[key]], 0) + prior_count
#avg <- numerator / denominator
```

##### Simulation

Simulation creates an plain of cells that all start from intact condition. Then agents start to operate in them, iteratively choosing actions and optimising rewards as described above. Here we simulate 100 steps per agent x 200 independent runs. 

```{r simulations abpes, echo=TRUE}
simulate_adaptive <- function(steps = 100) {
  land_grid <- all_cells %>% mutate(rangeland = "intact") #assumes that all cells start from an intact condition = which is naive
  agent1 <- create_agent("A1")
  agent2 <- create_agent("A2")
  
  logs <- list()
  weather_trace <- character(steps)  # Store weather for debugging
  
  for (t in 1:steps) {
    weather <- draw_weather()
    weather_trace[t] <- weather
    cat(sprintf("Step %d — Weather: %s\n", t, weather))
    
    a1 <- select_action(agent1)
    a2 <- select_action(agent2)
    
    same_cell <- a1$x == a2$x && a1$y == a2$y
    
    a1_state <- land_grid %>% filter(x == a1$x, y == a1$y) %>% pull(rangeland)
    a2_state <- land_grid %>% filter(x == a2$x, y == a2$y) %>% pull(rangeland)
    
    a1_revisit <- isTRUE(agent1$last_cell["x"] == a1$x &&
                           agent1$last_cell["y"] == a1$y)
    a2_revisit <- isTRUE(agent2$last_cell["x"] == a2$x &&
                           agent2$last_cell["y"] == a2$y)
    
    r1 <- compute_reward(a1$action, same_cell, a1_revisit, a1_state)
    r2 <- compute_reward(a2$action, same_cell, a2_revisit, a2_state)
    
    new1 <- transition_rangeland(a1_state, a1$action, weather)
    new2 <- transition_rangeland(a2_state, a2$action, weather)
    
    land_grid[land_grid$x == a1$x & land_grid$y == a1$y, "rangeland"] <- new1
    land_grid[land_grid$x == a2$x & land_grid$y == a2$y, "rangeland"] <- new2
    
    logs[[t]] <- list(
      step = t,
      weather = weather,
      land_grid = land_grid,
      a1 = a1,
      a2 = a2
    )
  }
  
  return(list(logs = logs, weather_history = weather_trace))
}

```

```{r simulation result abpes, include=FALSE}
set.seed(128)
n_runs  <- 200
n_steps <- 100

results_list <- map(1:n_runs, ~ simulate_adaptive(steps = n_steps))
```


#### Results

The landscape degradation rate summary graph below suggests that at the end of the 100 years, nearly 70% of the landscape would be degraded if assumptions about actions, climatic impacts and parameters in transition matrix were true. Under action based PES scenario, 50% of landscape is degraded in about 55 years based on the average of 200 independent simulation runs. 

```{r data reshape abpes, echo=FALSE, warning=FALSE}
# --- 2) Build % degraded per step per run ------------------------------------
degradation_by_run <- map2_dfr(
  results_list, seq_along(results_list),
  function(res, sim_id) {
    # For each step, compute % degraded across the grid
    purrr::map_dfr(res$logs, ~ .x$land_grid %>%
                     mutate(step = .x$step,
                            degraded = rangeland == "degraded")) %>%
      group_by(step) %>%
      summarise(pct_degraded = mean(degraded) * 100, .groups = "drop") %>%
      mutate(sim = sim_id)
  }
)

# --- 3) Mean and 95% interval across runs ------------------------------------
degradation_mean <- degradation_by_run %>%
  group_by(step) %>%
  summarise(pct_degraded = mean(pct_degraded), .groups = "drop")

degradation_ci <- degradation_by_run %>%
  group_by(step) %>%
  summarise(
    mean = mean(pct_degraded),
    lo   = quantile(pct_degraded, 0.025),
    hi   = quantile(pct_degraded, 0.975),
    .groups = "drop"
  )

# --- 4) Plot: all runs (grey), mean (solid), 95% ribbon ----------------------
ggplot() +
  geom_line(
    data = degradation_by_run,
    aes(x = step, y = pct_degraded, group = sim),
    color = "grey70", alpha = 0.45
  ) +
  geom_ribbon(
    data = degradation_ci,
    aes(x = step, ymin = lo, ymax = hi),
    fill = "firebrick", alpha = 0.12
  ) +
  geom_line(
    data = degradation_mean,
    aes(x = step, y = pct_degraded),
    color = "firebrick", linewidth = 1.2
  ) +    geom_hline(yintercept = 50)+
  labs(
    title = "Action based PES degradation over time — 200 runs (mean ± 95% CI)",
    x = "Step", y = "% Degraded"
  ) +
  theme_minimal(base_size = 12)
```


Policy mix by cell by each agent

```{r data reshape policy mix abpes, echo=FALSE, warning=FALSE}
action_df_all <- map2_dfr(
  results_list, seq_along(results_list),
  function(results, sim_id) {
    # Build a per-step action frame for THIS run
    map2_dfr(results$logs, results$weather_history, function(res, weather) {
      tibble(
        sim     = sim_id,
        step    = res$step,
        weather = weather,
        x1 = res$a1$x, y1 = res$a1$y, a1 = res$a1$action,
        x2 = res$a2$x, y2 = res$a2$y, a2 = res$a2$action
      )
    })
  }
)

action_long_all <- action_df_all %>%
  pivot_longer(
    cols = c(x1, y1, a1, x2, y2, a2),
    names_to = c(".value", "agent"),
    names_pattern = "([a-z]+)([12])"
  ) %>%
  mutate(agent = paste0("A", agent)) %>%
  rename(action = a)

policy_map_weather <- action_long_all %>%
  group_by(agent, x, y, weather, action) %>%
  summarise(n = n(), .groups = "drop_last") %>%
  slice_max(n, n = 1, with_ties = FALSE) %>%
  ungroup()

high_actions <- c("400-600", ">600")
low_actions  <- c("<100", "200-400")

# compute % of cells in each bin per facet (agent × weather)
summary_labels <- policy_map_weather %>%
  group_by(agent, weather) %>%
  summarise(
    n_cells = n_distinct(x, y),
    high = sum(action %in% high_actions),
    low  = sum(action %in% low_actions),
    high_pct = 100 * high / n_cells,
    low_pct  = 100 * low  / n_cells,
    .groups = "drop"
  ) %>%
  mutate(
    label = paste0(
      "High: ", sprintf("%.1f", high_pct), "%\n",
      "Low: ",  sprintf("%.1f", low_pct),  "%"
    )
  )

# find panel bounds to position label “on top”
x_min <- min(policy_map_weather$x)
x_max <- max(policy_map_weather$x)
y_max <- max(policy_map_weather$y)

# plot with labels
ggplot(policy_map_weather, aes(x = x, y = y, fill = action)) +
  geom_tile(color = "white") +
  facet_grid(agent ~ weather) +
  scale_fill_brewer(palette = "Set2") +
  coord_fixed(clip = "off") +
  # add a little extra headroom so labels sit “on top” of the grid
  expand_limits(y = y_max + 0.8) +
  # place the label near the top-left of each panel
  geom_text(
    data = summary_labels,
    aes(
      x = x_min,              # left edge
      y = y_max + 0.55,       # just above the grid
      label = label
    ),
    inherit.aes = FALSE,
    hjust = 0, vjust = 1,
    size = 3.8, lineheight = 1.05, color = "gray20"
  ) +
  labs(
    title = "Dominant Stocking Rate by Agents under different climate conditions",
    x = "Grid X", y = "Grid Y"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(10, 20, 10, 10),
    strip.background = element_rect(fill = "grey90", color = NA)
  )
```


### **II. Outcome based design **

##### Problem formulation
Outcome based PES is an incentive design that rewards the outcome of land use actions, rather than just the actions (action based PES). In this case, the outcome is the 4 rangeland states that are the results of the chosen actions/goat herd size and climatic conditions. The rest of the model structure is very similar to action based PES, the only difference being that rewards from incentives (green numbers in diagram). Herders still benefit the cash rewards from the herd directly (pink numbers in diagram). 

```{r conceptual mode outpes, echo=FALSE,include=TRUE}
knitr::include_graphics("outcome_based_pes_27Aug.jpg")
```

##### Difference between action and outcome based PES

Instead of rewards being the function of herd size alone (described under action based PES), here the incentives are based on rangeland states. For higher quality rangeland states herders are rewarded generously, whilst degraded state results in negative incentive (-1).  

```{r problem formulation outpes}
land_grid_x <- 5
land_grid_y <- 5
all_cells <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1))

# Actions (stocking rates)
actions <- c("<100", "200-400", "400-600", ">600")
action_income <- c("<100" = 1, "200-400" = 2, "400-600" = 3, ">600" = 4)

# Rangeland states
r_states <- c("intact", "good", "poor", "degraded")
r_code <- setNames(0:3, r_states)
r_income<-c("intact"=4,"good"=3,"poor"=2,"degraded"=-1)

# Weather probabilities
weather_probs <- c("drought" = 0.3, "rainy" = 0.2, "normal" = 0.5)

#Weather, transition and reward functions
#---------------------------------------------------------------------------
draw_weather <- function() {
  sample(names(weather_probs), size=1, prob = weather_probs)
}
```



```{r transition matrix outpes, echo=FALSE}
transition_rangeland <- function(state, action, weather) {
  if (state == "degraded") return("degraded")  # terminal state
  
  probs <- NULL
  
  # Define probabilities based on action and weather
  if (action == ">600") {
    if (weather == "drought") {
      probs <- c("degraded" = 0.9, "poor" = 0.1)
    } else if (weather == "normal") {
      probs <- c("degraded" = 0.7, "poor" = 0.3)
    } else {  # rainy
      probs <- c("poor" = 0.8, "good" = 0.2)
    }
    
  } else if (action == "400-600") {
    if (weather == "drought") {
      probs <- c("poor" = 0.6, "good" = 0.4)
    } else if (weather == "normal") {
      probs <- c("good" = 0.7, "intact" = 0.3)
    } else {
      probs <- c("good" = 0.4, "intact" = 0.6)
    }
    
  } else if (action == "200-400") {
    if (weather == "drought") {
      probs <- c("good" = 0.3, "intact" = 0.7)
    } else if (weather == "normal") {
      probs <- c("good" = 0.1, "intact" = 0.9)
    } else {
      probs <- c("intact" = 1.0)
    }
    
  } else {  # action == "<100"
    probs <- c("intact" = 1.0)
  }
  
  return(sample(names(probs), 1, prob = probs))
}
```


##### 2. Agents, actions and rewards
Here you will see that income from goat herd and incentives from PES are combined/added in total reward (r). 
```{r compute return outpes, echo=TRUE}
compute_reward <- function(action, same_cell, revisit,cell_condition) {
  r <- action_income[action]+r_income[action]
  if (same_cell) r <- r * 0.7
  if (revisit) r <- r * 0.7
  
  #A rangeland condition multiplier in reward (with the assumption that herders would benefit from a good quality pasture)
  condition_modifier <- dplyr::case_when(
    cell_condition == "intact"   ~ 1.0,
    cell_condition == "good"     ~ 0.9,
    cell_condition == "poor"     ~ 0.7,
    cell_condition == "degraded" ~ 0.4,
    TRUE ~ 1.0
  )
  
  r <- r * condition_modifier
  return(r)
}

softmax <- function(x, tau = 1.0) {
  exp_x <- exp((x - max(x)) / tau)
  exp_x / sum(exp_x)
}
create_agent <- function(name, tau = 1.0) {
  list(
    name = name,
    location = sample_n(all_cells, 1),
    last_cell = NA,
    reward_memory = list(),
    memory_count = list(),
    tau = tau
  )
}
```

The transition matrix for outcome based PES is exactly the same as action based PES described above. Agents also employ the same strategy as action based PES. 

```{r action selection outpes, echo=FALSE}
select_action <- function(agent) {
  candidates <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1), action = actions)
  candidates$reward <- apply(candidates, 1, function(row) {
    key <- paste(row[["x"]], row[["y"]], row[["action"]], sep = "_")
    if (!is.null(agent$reward_memory[[key]]) && agent$memory_count[[key]] > 0) {
      avg <- agent$reward_memory[[key]] / agent$memory_count[[key]]
    } else {
      avg <- 2 
    }
   
    # Safe revisit check
    same_as_last_cell <- isTRUE(agent$last_cell["x"] == as.numeric(row[["x"]]) &&
                                  agent$last_cell["y"] == as.numeric(row[["y"]]))
    if (same_as_last_cell) avg <- avg * 0.7
    return(avg)
  })
  
  probs <- softmax(candidates$reward, agent$tau)
  choice <- sample(1:nrow(candidates), 1, prob = probs)
  return(candidates[choice, ])
}
```


```{r simulations outpes, echo=FALSE, include =FALSE}
simulate_adaptive <- function(steps = 100) {
  land_grid <- all_cells %>% mutate(rangeland = "intact") #assumes that all cells start from an intact condition = which is naive
  agent1 <- create_agent("A1")
  agent2 <- create_agent("A2")
  
  logs <- list()
  weather_trace <- character(steps)  # Store weather for debugging
  
  for (t in 1:steps) {
    weather <- draw_weather()
    weather_trace[t] <- weather
    cat(sprintf("Step %d — Weather: %s\n", t, weather))
    
    a1 <- select_action(agent1)
    a2 <- select_action(agent2)
    
    same_cell <- a1$x == a2$x && a1$y == a2$y
    
    a1_state <- land_grid %>% filter(x == a1$x, y == a1$y) %>% pull(rangeland)
    a2_state <- land_grid %>% filter(x == a2$x, y == a2$y) %>% pull(rangeland)
    
    a1_revisit <- isTRUE(agent1$last_cell["x"] == a1$x &&
                           agent1$last_cell["y"] == a1$y)
    a2_revisit <- isTRUE(agent2$last_cell["x"] == a2$x &&
                           agent2$last_cell["y"] == a2$y)
    
    r1 <- compute_reward(a1$action, same_cell, a1_revisit, a1_state)
    r2 <- compute_reward(a2$action, same_cell, a2_revisit, a2_state)
    
    new1 <- transition_rangeland(a1_state, a1$action, weather)
    new2 <- transition_rangeland(a2_state, a2$action, weather)
    
    land_grid[land_grid$x == a1$x & land_grid$y == a1$y, "rangeland"] <- new1
    land_grid[land_grid$x == a2$x & land_grid$y == a2$y, "rangeland"] <- new2

    
    logs[[t]] <- list(
      step = t,
      weather = weather,
      land_grid = land_grid,
      a1 = a1,
      a2 = a2
    )
  }
  
  return(list(logs = logs, weather_history = weather_trace))
}

```

```{r simulation result outpes, include=FALSE}
set.seed(121)
n_runs  <- 200
n_steps <- 100

results_list <- map(1:n_runs, ~ simulate_adaptive(steps = n_steps))
```


#### Results

Ecosystem degradation rate is similar to Action based PES. The 50% degradation threshold is reached in about 59 years.  

```{r data reshape outpes, echo=FALSE, warning=FALSE}
# --- 2) Build % degraded per step per run ------------------------------------
degradation_by_run <- map2_dfr(
  results_list, seq_along(results_list),
  function(res, sim_id) {
    # For each step, compute % degraded across the grid
    purrr::map_dfr(res$logs, ~ .x$land_grid %>%
                     mutate(step = .x$step,
                            degraded = rangeland == "degraded")) %>%
      group_by(step) %>%
      summarise(pct_degraded = mean(degraded) * 100, .groups = "drop") %>%
      mutate(sim = sim_id)
  }
)

# --- 3) Mean and 95% interval across runs ------------------------------------
degradation_mean <- degradation_by_run %>%
  group_by(step) %>%
  summarise(pct_degraded = mean(pct_degraded), .groups = "drop")

degradation_ci <- degradation_by_run %>%
  group_by(step) %>%
  summarise(
    mean = mean(pct_degraded),
    lo   = quantile(pct_degraded, 0.025),
    hi   = quantile(pct_degraded, 0.975),
    .groups = "drop"
  )

# --- 4) Plot: all runs (grey), mean (solid), 95% ribbon ----------------------
ggplot() +
  geom_line(
    data = degradation_by_run,
    aes(x = step, y = pct_degraded, group = sim),
    color = "grey70", alpha = 0.45
  ) +
  geom_ribbon(
    data = degradation_ci,
    aes(x = step, ymin = lo, ymax = hi),
    fill = "orange", alpha = 0.12
  ) +
  geom_line(
    data = degradation_mean,
    aes(x = step, y = pct_degraded),
    color = "orange", linewidth = 1.2
  ) +    geom_hline(yintercept = 50)+
  labs(
    title = "Outcome based PES degradation over time — 200 runs (mean ± 95% CI)",
    x = "Step", y = "% Degraded"
  ) +
  theme_minimal(base_size = 12)
```


Policy mix by cell by each agent

```{r data reshape policy mix outpes, echo=FALSE, warning=FALSE}
action_df_all <- map2_dfr(
  results_list, seq_along(results_list),
  function(results, sim_id) {
    # Build a per-step action frame for THIS run
    map2_dfr(results$logs, results$weather_history, function(res, weather) {
      tibble(
        sim     = sim_id,
        step    = res$step,
        weather = weather,
        x1 = res$a1$x, y1 = res$a1$y, a1 = res$a1$action,
        x2 = res$a2$x, y2 = res$a2$y, a2 = res$a2$action
      )
    })
  }
)

action_long_all <- action_df_all %>%
  pivot_longer(
    cols = c(x1, y1, a1, x2, y2, a2),
    names_to = c(".value", "agent"),
    names_pattern = "([a-z]+)([12])"
  ) %>%
  mutate(agent = paste0("A", agent)) %>%
  rename(action = a)

policy_map_weather <- action_long_all %>%
  group_by(agent, x, y, weather, action) %>%
  summarise(n = n(), .groups = "drop_last") %>%
  slice_max(n, n = 1, with_ties = FALSE) %>%
  ungroup()

high_actions <- c("400-600", ">600")
low_actions  <- c("<100", "200-400")

# compute % of cells in each bin per facet (agent × weather)
summary_labels <- policy_map_weather %>%
  group_by(agent, weather) %>%
  summarise(
    n_cells = n_distinct(x, y),
    high = sum(action %in% high_actions),
    low  = sum(action %in% low_actions),
    high_pct = 100 * high / n_cells,
    low_pct  = 100 * low  / n_cells,
    .groups = "drop"
  ) %>%
  mutate(
    label = paste0(
      "High: ", sprintf("%.1f", high_pct), "%\n",
      "Low: ",  sprintf("%.1f", low_pct),  "%"
    )
  )

# find panel bounds to position label “on top”
x_min <- min(policy_map_weather$x)
x_max <- max(policy_map_weather$x)
y_max <- max(policy_map_weather$y)

# plot with labels
ggplot(policy_map_weather, aes(x = x, y = y, fill = action)) +
  geom_tile(color = "white") +
  facet_grid(agent ~ weather) +
  scale_fill_brewer(palette = "Set2") +
  coord_fixed(clip = "off") +
  # add a little extra headroom so labels sit “on top” of the grid
  expand_limits(y = y_max + 0.8) +
  # place the label near the top-left of each panel
  geom_text(
    data = summary_labels,
    aes(
      x = x_min,              # left edge
      y = y_max + 0.55,       # just above the grid
      label = label
    ),
    inherit.aes = FALSE,
    hjust = 0, vjust = 1,
    size = 3.8, lineheight = 1.05, color = "gray20"
  ) +
  labs(
    title = "Dominant Stocking Rate by Agents under different climate conditions",
    x = "Grid X", y = "Grid Y"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(10, 20, 10, 10),
    strip.background = element_rect(fill = "grey90", color = NA)
  )
```

### **III. Null model - No PES **

##### Problem formulation
In contrast to action and outcome based PES described above, the "Null" model describes a base scenario, where there is no incentive. Agents still get cash rewards for direct benefits from herding, just like in real life. However, there is no additional reward from an incentive scheme (lack of green colored rewards in Figure 3). The significance of this model is to test additionality of incentives from action based and outcome based PES described in sections I and II. 

```{r conceptual mode nopes, echo=FALSE,include=TRUE}
knitr::include_graphics("No_incentive_for_additionality_27Aug.jpg")
```

No incentive model is set up very similarly to Action and Outcome based PES models, apart from the reward structure. The only rewards that herders get are from goat rearing described in action_income string.  

```{r problem formulation nopes}

land_grid_x <- 5
land_grid_y <- 5
all_cells <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1))

# Actions (stocking rates)
actions <- c("<100", "200-400", "400-600", ">600")
action_income <- c("<100" = 1, "200-400" = 2, "400-600" = 3, ">600" = 4)

# Rangeland states
r_states <- c("intact", "good", "poor", "degraded")
r_code <- setNames(0:3, r_states)

# Weather probabilities
weather_probs <- c("drought" = 0.3, "rainy" = 0.2, "normal" = 0.5)

#Weather, transition and reward functions
#---------------------------------------------------------------------------
draw_weather <- function() {
  sample(names(weather_probs), size=1, prob = weather_probs)
}
```

The total financial income for a herder is described through the compute_reward function below through the action_income string. 

```{r compute return nopes, echo=TRUE}

compute_reward <- function(action, same_cell, revisit,cell_condition) {
  r <- action_income[action]
  if (same_cell) r <- r * 0.7
  if (revisit) r <- r * 0.7
  
  #A rangeland condition multiplier in reward (with the assumption that herders would benefit from a good quality pasture)
  condition_modifier <- dplyr::case_when(
    cell_condition == "intact"   ~ 1.0,
    cell_condition == "good"     ~ 0.9,
    cell_condition == "poor"     ~ 0.7,
    cell_condition == "degraded" ~ 0.4,
    TRUE ~ 1.0
  )
  
  r <- r * condition_modifier
  return(r)
}

softmax <- function(x, tau = 1.0) {
  exp_x <- exp((x - max(x)) / tau)
  exp_x / sum(exp_x)
}
create_agent <- function(name, tau = 1.0) {
  list(
    name = name,
    location = sample_n(all_cells, 1),
    last_cell = NA,
    reward_memory = list(),
    memory_count = list(),
    tau = tau
  )
}
```

Otherwise,climate effects and the transition matrix in No PES model are identical to the action and outcome based incentive designs. Action selection and the spatial coordination strategies are also identical to action and outcome based schemes. 

No incentive model is set up very similarly to Action and Outcome based PES models, in that climate effects and the transition matrix are identical structure to the two incentive designs. 
```{r transition matrix nopes, echo=FALSE, include=FALSE}

transition_rangeland <- function(state, action, weather) {
  if (state == "degraded") return("degraded")  # terminal state
  
  probs <- NULL
  
  # Define probabilities based on action and weather
  if (action == ">600") {
    if (weather == "drought") {
      probs <- c("degraded" = 0.9, "poor" = 0.1)
    } else if (weather == "normal") {
      probs <- c("degraded" = 0.7, "poor" = 0.3)
    } else {  # rainy
      probs <- c("poor" = 0.8, "good" = 0.2)
    }
    
  } else if (action == "400-600") {
    if (weather == "drought") {
      probs <- c("poor" = 0.6, "good" = 0.4)
    } else if (weather == "normal") {
      probs <- c("good" = 0.7, "intact" = 0.3)
    } else {
      probs <- c("good" = 0.4, "intact" = 0.6)
    }
    
  } else if (action == "200-400") {
    if (weather == "drought") {
      probs <- c("good" = 0.3, "intact" = 0.7)
    } else if (weather == "normal") {
      probs <- c("good" = 0.1, "intact" = 0.9)
    } else {
      probs <- c("intact" = 1.0)
    }
    
  } else {  # action == "<100"
    probs <- c("intact" = 1.0)
  }
  
  return(sample(names(probs), 1, prob = probs))
}
```


```{r action selection nopes, echo=FALSE}

select_action <- function(agent) {
  candidates <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1), action = actions)
  candidates$reward <- apply(candidates, 1, function(row) {
    key <- paste(row[["x"]], row[["y"]], row[["action"]], sep = "_")
    if (!is.null(agent$reward_memory[[key]]) && agent$memory_count[[key]] > 0) {
      avg <- agent$reward_memory[[key]] / agent$memory_count[[key]]
    } else {
      avg <- 2 
    }
   
    # Safe revisit check
    same_as_last_cell <- isTRUE(agent$last_cell["x"] == as.numeric(row[["x"]]) &&
                                  agent$last_cell["y"] == as.numeric(row[["y"]]))
    if (same_as_last_cell) avg <- avg * 0.7
    return(avg)
  })
  
  probs <- softmax(candidates$reward, agent$tau)
  choice <- sample(1:nrow(candidates), 1, prob = probs)
  return(candidates[choice, ])
}
```


```{r simulations nopes, echo=FALSE, include =FALSE}
simulate_adaptive <- function(steps = 100) {
  land_grid <- all_cells %>% mutate(rangeland = "intact") #assumes that all cells start from an intact condition = which is naive
  agent1 <- create_agent("A1")
  agent2 <- create_agent("A2")
  
  logs <- list()
  weather_trace <- character(steps)  # Store weather for debugging
  
  for (t in 1:steps) {
    weather <- draw_weather()
    weather_trace[t] <- weather
    cat(sprintf("Step %d — Weather: %s\n", t, weather))
    
    a1 <- select_action(agent1)
    a2 <- select_action(agent2)
    
    same_cell <- a1$x == a2$x && a1$y == a2$y
    
    a1_state <- land_grid %>% filter(x == a1$x, y == a1$y) %>% pull(rangeland)
    a2_state <- land_grid %>% filter(x == a2$x, y == a2$y) %>% pull(rangeland)
    
    a1_revisit <- isTRUE(agent1$last_cell["x"] == a1$x &&
                           agent1$last_cell["y"] == a1$y)
    a2_revisit <- isTRUE(agent2$last_cell["x"] == a2$x &&
                           agent2$last_cell["y"] == a2$y)
    
    r1 <- compute_reward(a1$action, same_cell, a1_revisit, a1_state)
    r2 <- compute_reward(a2$action, same_cell, a2_revisit, a2_state)
    
    new1 <- transition_rangeland(a1_state, a1$action, weather)
    new2 <- transition_rangeland(a2_state, a2$action, weather)
    
    land_grid[land_grid$x == a1$x & land_grid$y == a1$y, "rangeland"] <- new1
    land_grid[land_grid$x == a2$x & land_grid$y == a2$y, "rangeland"] <- new2

    
    logs[[t]] <- list(
      step = t,
      weather = weather,
      land_grid = land_grid,
      a1 = a1,
      a2 = a2
    )
  }
  
  return(list(logs = logs, weather_history = weather_trace))
}

```

```{r simulation result nopes, include=FALSE}
set.seed(129)
n_runs  <- 200
n_steps <- 100
results_list <- map(1:n_runs, ~ simulate_adaptive(steps = n_steps))

```


#### Results

Ecosystem degradation rate is similar to Action Based PES. The dominant stocking rate seems to suggest the most common herd size herders choose when there is no PES is generally <400 goats (14/25 cells). 

```{r data reshape nopes, echo=FALSE, warning=FALSE}
degradation_by_run <- map2_dfr(
  results_list, seq_along(results_list),
  function(res, sim_id) {
    # For each step, compute % degraded across the grid
    purrr::map_dfr(res$logs, ~ .x$land_grid %>%
                     mutate(step = .x$step,
                            degraded = rangeland == "degraded")) %>%
      group_by(step) %>%
      summarise(pct_degraded = mean(degraded) * 100, .groups = "drop") %>%
      mutate(sim = sim_id)
  }
)

# --- 3) Mean and 95% interval across runs ------------------------------------
degradation_mean <- degradation_by_run %>%
  group_by(step) %>%
  summarise(pct_degraded = mean(pct_degraded), .groups = "drop")

degradation_ci <- degradation_by_run %>%
  group_by(step) %>%
  summarise(
    mean = mean(pct_degraded),
    lo   = quantile(pct_degraded, 0.025),
    hi   = quantile(pct_degraded, 0.975),
    .groups = "drop"
  )

# --- 4) Plot: all runs (grey), mean (solid), 95% ribbon ----------------------
ggplot() +
  geom_line(
    data = degradation_by_run,
    aes(x = step, y = pct_degraded, group = sim),
    color = "grey70", alpha = 0.45
  ) +
  geom_ribbon(
    data = degradation_ci,
    aes(x = step, ymin = lo, ymax = hi),
    fill = "skyblue", alpha = 0.12
  ) +
  geom_line(
    data = degradation_mean,
    aes(x = step, y = pct_degraded),
    color = "skyblue", linewidth = 1.2
  ) +    geom_hline(yintercept = 50)+
  labs(
    title = "No PES degradation over time— 200 runs (mean ± 95% CI)",
    x = "Step", y = "% Degraded"
  ) +
  theme_minimal(base_size = 12)
```


```{r data reshape policy mix nopes, echo=FALSE, warning=FALSE}
action_df_all <- map2_dfr(
  results_list, seq_along(results_list),
  function(results, sim_id) {
    # Build a per-step action frame for THIS run
    map2_dfr(results$logs, results$weather_history, function(res, weather) {
      tibble(
        sim     = sim_id,
        step    = res$step,
        weather = weather,
        x1 = res$a1$x, y1 = res$a1$y, a1 = res$a1$action,
        x2 = res$a2$x, y2 = res$a2$y, a2 = res$a2$action
      )
    })
  }
)

action_long_all <- action_df_all %>%
  pivot_longer(
    cols = c(x1, y1, a1, x2, y2, a2),
    names_to = c(".value", "agent"),
    names_pattern = "([a-z]+)([12])"
  ) %>%
  mutate(agent = paste0("A", agent)) %>%
  rename(action = a)

policy_map_weather <- action_long_all %>%
  group_by(agent, x, y, weather, action) %>%
  summarise(n = n(), .groups = "drop_last") %>%
  slice_max(n, n = 1, with_ties = FALSE) %>%
  ungroup()

high_actions <- c("400-600", ">600")
low_actions  <- c("<100", "200-400")

# compute % of cells in each bin per facet (agent × weather)
summary_labels <- policy_map_weather %>%
  group_by(agent, weather) %>%
  summarise(
    n_cells = n_distinct(x, y),
    high = sum(action %in% high_actions),
    low  = sum(action %in% low_actions),
    high_pct = 100 * high / n_cells,
    low_pct  = 100 * low  / n_cells,
    .groups = "drop"
  ) %>%
  mutate(
    label = paste0(
      "High: ", sprintf("%.1f", high_pct), "%\n",
      "Low: ",  sprintf("%.1f", low_pct),  "%"
    )
  )

# find panel bounds to position label “on top”
x_min <- min(policy_map_weather$x)
x_max <- max(policy_map_weather$x)
y_max <- max(policy_map_weather$y)

# plot with labels
ggplot(policy_map_weather, aes(x = x, y = y, fill = action)) +
  geom_tile(color = "white") +
  facet_grid(agent ~ weather) +
  scale_fill_brewer(palette = "Set2") +
  coord_fixed(clip = "off") +
  # add a little extra headroom so labels sit “on top” of the grid
  expand_limits(y = y_max + 0.8) +
  # place the label near the top-left of each panel
  geom_text(
    data = summary_labels,
    aes(
      x = x_min,              # left edge
      y = y_max + 0.55,       # just above the grid
      label = label
    ),
    inherit.aes = FALSE,
    hjust = 0, vjust = 1,
    size = 3.8, lineheight = 1.05, color = "gray20"
  ) +
  labs(
    title = "Dominant Stocking Rate by Agents under different climate conditions",
    x = "Grid X", y = "Grid Y"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.margin = margin(10, 20, 10, 10),
    strip.background = element_rect(fill = "grey90", color = NA)
  )

```
