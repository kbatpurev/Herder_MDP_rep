---
title: "Proof of concept - Exploring human dimensions of market-based incentive design with reinfocement
  learning"
output:
  html_document: default
  pdf_document: default
date: "2025-08-07"
---

### Markov Decision Processes to predict ecosystem condition in Mongolia under different herding practices

*Markov Decision Processes (MDPs) is a probabilistic framework that is computes the optimal course of action in a repeated-decisions scenario through iterative agent based learning. It belongs to a family of machine learning algorithms called Reinforcement Learning. Here we apply MDPs to explore how herder behaviour and practices affect ecosystem condition in Mongolian rangelands. We approach this problem as a spatially explicit multi-agent reinforcement learning problem (MARL) with herders as agents, goat rearing as actions, and environmental conditions as states and climate as a stochastic variable that impacts both actions and states.*


#### Github repository

###### https://github.com/kbatpurev/Herder_MDP_rep.git

##### Document structure 

###### (V1. Toy version with two agents with softmax policy)

1. Model 1 - Action based design
2. Model 2 - Outcome based design
3. Null model - No incentive 

======================================================================================================================
```{r setup, include=FALSE}
.libPaths()
.libPaths(new="C:/Data/RStudio")
library(jsonlite)
library(highr)
library(tidyverse)
library(dplyr)
library(quantmod) # for findPeaks fxn
library(MDPtoolbox)

# plotting libraries
library(corrplot)
library(RColorBrewer)
library(ggthemes)
library(ggpubr)
library(ggplot2)
library(Cairo)
library(extrafont)
extrafont::loadfonts()
library(kableExtra)
library(truncnorm)
```

### **I. Action based PES**

Action based PES is when incentives are rewarded for actions that land owners or herders take to reduce land use pressures on ecosystem, in this case reducing or maintaining the goat herd size to improve and or maintain rangeland condition. We define 4 different actions: choosing to have herd sizes a)less than 100,b) 200-400, c)400-600 and d)600 and above. Each of these actions bring rewards or income to varying degrees. The rangeland can be in one of four states: intact, good, poor or degraded. And the weather conditions are drawn from a probability distribution where a given year has 50% of being in average condition, 30% chance of being in drought condition and 20% chance of being rainy (higher than average rainfall). Here is a conceptual model of the action based design that we explore in this chapter. In this model, the PES incentives are described with green numbers associated with the 4 actions. Whilst the direct cash profit associated with each action is described in brown. 

```{r conceptual model abpes,echo=FALSE,include=TRUE}
knitr::include_graphics("action_based_pes_27Aug.jpg")

```

##### Problem formulation
Here we create a 5 x 5 raster where two agents herd goats in the same space.

```{r problem formulation abpes}
land_grid_x <- 5
land_grid_y <- 5
all_cells <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1))

# Actions (stocking rates)
actions <- c("<100", "200-400", "400-600", ">600")
action_income <- c("<100" = 0.8, "200-400" = 2.0, "400-600" = 2.5, ">600" = 3.5)

# Rangeland states
r_states <- c("intact", "good", "poor", "degraded")
r_code <- setNames(0:3, r_states)

# Weather probabilities
weather_probs <- c("drought" = 0.3, "rainy" = 0.2, "normal" = 0.5)

#Weather, transition and reward functions
#---------------------------------------------------------------------------
draw_weather <- function() {
  sample(names(weather_probs), size=1, prob = weather_probs)
}
```

##### Transition matrix
This is a long if then statement that defines how rangeland states relate to each action and weather condition combination. This needs to be coded up more efficiently for a hyper parameter grid search later on. But for now it is more of an illustration tool of how we perceive actions and climatic conditions impact the rangeland states. 
```{r transition matrix abpes ,echo=TRUE}
transition_rangeland <- function(state, action, weather) {
  if (state == "degraded") return("degraded")  # terminal state
  
  probs <- NULL
  
  # Define probabilities based on action and weather
  if (action == ">600") {
    if (weather == "drought") {
      probs <- c("degraded" = 0.9, "poor" = 0.1)
    } else if (weather == "normal") {
      probs <- c("degraded" = 0.7, "poor" = 0.3)
    } else {  # rainy
      probs <- c("poor" = 0.8, "good" = 0.2)
    }
    
  } else if (action == "400-600") {
    if (weather == "drought") {
      probs <- c("poor" = 0.6, "good" = 0.4)
    } else if (weather == "normal") {
      probs <- c("good" = 0.7, "intact" = 0.3)
    } else {
      probs <- c("good" = 0.4, "intact" = 0.6)
    }
    
  } else if (action == "200-400") {
    if (weather == "drought") {
      probs <- c("good" = 0.3, "intact" = 0.7)
    } else if (weather == "normal") {
      probs <- c("good" = 0.1, "intact" = 0.9)
    } else {
      probs <- c("intact" = 1.0)
    }
    
  } else {  # action == "<100"
    probs <- c("intact" = 1.0)
  }
  
  return(sample(names(probs), 1, prob = probs))
}
```


##### 2. Agents, actions and rewards

##### Reward structure and policy
Here we define how income from actions (rewards) are calculated. Rewards are a function of herd size (type of action), actions of the other agent. The specific strategy an agent employs to weigh the pros and cons of each reward is called policy, in this case this strategy is a "softmax" approach which is to maximise rewards with a moderate tau (temperature) of 1. Temperature settings for the policy can vary between 2 and 0. 

###### *Lower tau → more greedy (e.g., 0.1)*
###### *Higher tau → more exploratory (e.g., 2.0)*
###### *tau = 1 is a moderate setting (default in many systems)*

```{r compute return abpes}
compute_reward <- function(action, same_cell, revisit,cell_condition) {
  r <- action_income[action]
  if (same_cell) r <- r * 0.7
  if (revisit) r <- r * 0.7
  
  #A rangeland condition multiplier in reward (with the assumption that herders would benefit from a good quality pasture)
  condition_modifier <- dplyr::case_when(
    cell_condition == "intact"   ~ 1.0,
    cell_condition == "good"     ~ 0.9,
    cell_condition == "poor"     ~ 0.7,
    cell_condition == "degraded" ~ 0.4,
    TRUE ~ 1.0
  )
  
  r <- r * condition_modifier
  return(r)
}

softmax <- function(x, tau = 1.0) {
  exp_x <- exp((x - max(x)) / tau)
  exp_x / sum(exp_x)
}
create_agent <- function(name, tau = 1.0) {
  list(
    name = name,
    location = sample_n(all_cells, 1),
    last_cell = NA,
    reward_memory = list(),
    memory_count = list(),
    tau = tau
  )
}
```

##### Action selection in space

Agent selects actions depending on where they are in the landscape in relation to other agents - this is to emulate the preference for non overlapping grazing practices employed by herders, which only exists as a silent/social agreement and are often breached by outsiders and desperate herders. Agents also have to consider the last time they were in a cell, because they are penalised if they return to the same cell immediately - this is an attempt to emulate ecosystem recovery from land use. If an agent hasn't been to a cell before and never tried an action before then they assume reward to be an average of 2.This is clearly a naive and overly optimistic approach. Future simulation should focus on testing the sensitivity of this parameter.

```{r action selection abpes, echo=TRUE}
select_action <- function(agent) {
  candidates <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1), action = actions)
  candidates$reward <- apply(candidates, 1, function(row) {
    key <- paste(row[["x"]], row[["y"]], row[["action"]], sep = "_")
    if (!is.null(agent$reward_memory[[key]]) && agent$memory_count[[key]] > 0) {
      avg <- agent$reward_memory[[key]] / agent$memory_count[[key]]
    } else {
      avg <- 2 
    }
   
    # Safe revisit check
    same_as_last_cell <- isTRUE(agent$last_cell["x"] == as.numeric(row[["x"]]) &&
                                  agent$last_cell["y"] == as.numeric(row[["y"]]))
    if (same_as_last_cell) avg <- avg * 0.7
    return(avg)
  })
  
  probs <- softmax(candidates$reward, agent$tau)
  choice <- sample(1:nrow(candidates), 1, prob = probs)
  return(candidates[choice, ])
}
```

Alternative to using a constant of 2 as an average reward there is the option of using Bayesian smoothing based on prior knowledge through a small step like this: 

```{r alternative - Bayesian smoothing, eval=FALSE, include=FALSE}
#prior_mean <- 1.5
#prior_count <- 1
#numerator <- ifelse(!is.null(agent$reward_memory[[key]]), agent$reward_memory[[key]], 0) + prior_mean*prior_count
#denominator <- ifelse(!is.null(agent$memory_count[[key]]), agent$memory_count[[key]], 0) + prior_count
#avg <- numerator / denominator
```

##### Simulation

Simulation creates an plain of cells that all start from intact condition. Then agents start to operate in them, iteratively choosing actions and optimising rewards as described above. 
```{r simulations abpes, echo=TRUE}
simulate_adaptive <- function(steps = 100) {
  land_grid <- all_cells %>% mutate(rangeland = "intact") #assumes that all cells start from an intact condition = which is naive
  agent1 <- create_agent("A1")
  agent2 <- create_agent("A2")
  
  logs <- list()
  weather_trace <- character(steps)  # Store weather for debugging
  
  for (t in 1:steps) {
    weather <- draw_weather()
    weather_trace[t] <- weather
    cat(sprintf("Step %d — Weather: %s\n", t, weather))
    
    a1 <- select_action(agent1)
    a2 <- select_action(agent2)
    
    same_cell <- a1$x == a2$x && a1$y == a2$y
    
    a1_state <- land_grid %>% filter(x == a1$x, y == a1$y) %>% pull(rangeland)
    a2_state <- land_grid %>% filter(x == a2$x, y == a2$y) %>% pull(rangeland)
    
    a1_revisit <- isTRUE(agent1$last_cell["x"] == a1$x &&
                           agent1$last_cell["y"] == a1$y)
    a2_revisit <- isTRUE(agent2$last_cell["x"] == a2$x &&
                           agent2$last_cell["y"] == a2$y)
    
    r1 <- compute_reward(a1$action, same_cell, a1_revisit, a1_state)
    r2 <- compute_reward(a2$action, same_cell, a2_revisit, a2_state)
    
    new1 <- transition_rangeland(a1_state, a1$action, weather)
    new2 <- transition_rangeland(a2_state, a2$action, weather)
    
    land_grid[land_grid$x == a1$x & land_grid$y == a1$y, "rangeland"] <- new1
    land_grid[land_grid$x == a2$x & land_grid$y == a2$y, "rangeland"] <- new2
    
    #agent1 <- update_agent(agent1, a1, r1)
    #agent2 <- update_agent(agent2, a2, r2)
    
    logs[[t]] <- list(
      step = t,
      weather = weather,
      land_grid = land_grid,
      a1 = a1,
      a2 = a2
    )
  }
  
  return(list(logs = logs, weather_history = weather_trace))
}

```

```{r simulation result abpes, include=FALSE}
results <- simulate_adaptive(step=140)
```


##### Results

Ecosystem degradation rate over time
```{r data reshape abpes, echo=FALSE, warning=FALSE}
degradation_df <- map_dfr(results$logs, ~ .x$land_grid %>%
                            mutate(step = .x$step,
                                   degraded = rangeland == "degraded"))

degradation_summary <- degradation_df %>%
  group_by(step) %>%
  summarise(pct_degraded = mean(degraded) * 100)

ggplot(degradation_summary, aes(x = step, y = pct_degraded)) +
  geom_line(size = 1.2, color = "darkred") +
  labs(title = "Degradation Over Time", y = "% Degraded", x = "Step")
```


Policy mix by cell by each agent

```{r data reshape policy mix abpes, echo=FALSE, warning=FALSE}
action_df <- map2_dfr(results$logs, results$weather_history, function(res, weather) {
  tibble(
    step = res$step,
    weather = weather,
    x1 = res$a1$x,
    y1 = res$a1$y,
    a1 = res$a1$action,
    x2 = res$a2$x,
    y2 = res$a2$y,
    a2 = res$a2$action
  )
})
action_long <- action_df %>%
  pivot_longer(
    cols = c(x1, y1, a1, x2, y2, a2),
    names_to = c(".value", "agent"),
    names_pattern = "([a-z]+)([12])"
  ) %>%
  mutate(agent = paste0("A", agent))
action_long <- action_long %>% rename(action = a)

policy_map <- action_long %>%
  group_by(agent, x, y) %>%
  count(action) %>%
  slice_max(n, n = 1, with_ties = FALSE)

ggplot(policy_map, aes(x = x, y = y, fill = action)) +
  geom_tile(color = "white") +
  facet_wrap(~agent) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Dominant Stocking Rate by Agent", x = "land_grid X", y = "land_grid Y")
```


### **II. Outcome based design **

##### Problem formulation
Outcome based PES is an incentive design that rewards the outcome of land use actions, rather than just the actions (action based PES). In this case, the outcome is the 4 rangeland states that are the results of the chosen actions/goat herd size and climatic conditions. The rest of the model structure is very similar to action based PES, the only difference being that rewards from incentives (green numbers in diagram) are now associated with rangeland states. 

```{r conceptual mode outpes, echo=FALSE,include=TRUE}
knitr::include_graphics("outcome_based_pes_27Aug.jpg")
```

```{r problem formulation outpes}
land_grid_x <- 5
land_grid_y <- 5
all_cells <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1))

# Actions (stocking rates)
actions <- c("<100", "200-400", "400-600", ">600")
action_income <- c("<100" = 0.8, "200-400" = 2.0, "400-600" = 2.5, ">600" = 3.5)

# Rangeland states
r_states <- c("intact", "good", "poor", "degraded")
r_code <- setNames(0:3, r_states)

# Weather probabilities
weather_probs <- c("drought" = 0.3, "rainy" = 0.2, "normal" = 0.5)

#Weather, transition and reward functions
#---------------------------------------------------------------------------
draw_weather <- function() {
  sample(names(weather_probs), size=1, prob = weather_probs)
}
```

##### Transition matrix
This is a long if then statement that defines how rangeland states relate to each action and weather condition combination. This needs to be coded up more efficiently for a hyper parameter grid search later on. But for now it is more of an illustration tool of how we perceive actions and climatic conditions impact the rangeland states. 
```{r transition matrix outpes, echo=TRUE}
transition_rangeland <- function(state, action, weather) {
  if (state == "degraded") return("degraded")  # terminal state
  
  probs <- NULL
  
  # Define probabilities based on action and weather
  if (action == ">600") {
    if (weather == "drought") {
      probs <- c("degraded" = 0.9, "poor" = 0.1)
    } else if (weather == "normal") {
      probs <- c("degraded" = 0.7, "poor" = 0.3)
    } else {  # rainy
      probs <- c("poor" = 0.8, "good" = 0.2)
    }
    
  } else if (action == "400-600") {
    if (weather == "drought") {
      probs <- c("poor" = 0.6, "good" = 0.4)
    } else if (weather == "normal") {
      probs <- c("good" = 0.7, "intact" = 0.3)
    } else {
      probs <- c("good" = 0.4, "intact" = 0.6)
    }
    
  } else if (action == "200-400") {
    if (weather == "drought") {
      probs <- c("good" = 0.3, "intact" = 0.7)
    } else if (weather == "normal") {
      probs <- c("good" = 0.1, "intact" = 0.9)
    } else {
      probs <- c("intact" = 1.0)
    }
    
  } else {  # action == "<100"
    probs <- c("intact" = 1.0)
  }
  
  return(sample(names(probs), 1, prob = probs))
}
```


##### 2. Agents, actions and rewards

##### Reward structure and policy
Here we define how income from actions (rewards) are calculated. Rewards are a function of herd size (type of action), actions of the other agent. The specific strategy an agent employs to weigh the pros and cons of each reward is called policy, in this case this strategy is a "softmax" approach which is to maximise rewards with a moderate tau (temperature) of 1. Temperature settings for the policy can vary between 2 and 0. 

###### *Lower tau → more greedy (e.g., 0.1)*
###### *Higher tau → more exploratory (e.g., 2.0)*
###### *tau = 1 is a moderate setting (default in many systems)*

```{r compute return outpes}
compute_reward <- function(action, same_cell, revisit,cell_condition) {
  r <- action_income[action]
  if (same_cell) r <- r * 0.7
  if (revisit) r <- r * 0.7
  
  #A rangeland condition multiplier in reward (with the assumption that herders would benefit from a good quality pasture)
  condition_modifier <- dplyr::case_when(
    cell_condition == "intact"   ~ 1.0,
    cell_condition == "good"     ~ 0.9,
    cell_condition == "poor"     ~ 0.7,
    cell_condition == "degraded" ~ 0.4,
    TRUE ~ 1.0
  )
  
  r <- r * condition_modifier
  return(r)
}

softmax <- function(x, tau = 1.0) {
  exp_x <- exp((x - max(x)) / tau)
  exp_x / sum(exp_x)
}
create_agent <- function(name, tau = 1.0) {
  list(
    name = name,
    location = sample_n(all_cells, 1),
    last_cell = NA,
    reward_memory = list(),
    memory_count = list(),
    tau = tau
  )
}
```

##### Action selection in space

Agent selects actions depending on where they are in the landscape in relation to other agents - this is to emulate the preference for non overlapping grazing practices employed by herders, which only exists as a silent/social agreement and are often breached by outsiders and desperate herders. Agents also have to consider the last time they were in a cell, because they are penalised if they return to the same cell immediately - this is an attempt to emulate ecosystem recovery from land use. If an agent hasn't been to a cell before and never tried an action before then they assume reward to be an average of 2.This is clearly a naive and overly optimistic approach. Future simulation should focus on testing the sensitivity of this parameter.

```{r action selection outpes, echo=TRUE}
select_action <- function(agent) {
  candidates <- expand.grid(x = 0:(land_grid_x - 1), y = 0:(land_grid_y - 1), action = actions)
  candidates$reward <- apply(candidates, 1, function(row) {
    key <- paste(row[["x"]], row[["y"]], row[["action"]], sep = "_")
    if (!is.null(agent$reward_memory[[key]]) && agent$memory_count[[key]] > 0) {
      avg <- agent$reward_memory[[key]] / agent$memory_count[[key]]
    } else {
      avg <- 2 
    }
   
    # Safe revisit check
    same_as_last_cell <- isTRUE(agent$last_cell["x"] == as.numeric(row[["x"]]) &&
                                  agent$last_cell["y"] == as.numeric(row[["y"]]))
    if (same_as_last_cell) avg <- avg * 0.7
    return(avg)
  })
  
  probs <- softmax(candidates$reward, agent$tau)
  choice <- sample(1:nrow(candidates), 1, prob = probs)
  return(candidates[choice, ])
}
```

Alternative to using a constant of 2 as an average reward there is the option of using Bayesian smoothing based on prior knowledge through a small step like this: 

```{r alternative - Bayesian smoothing outpes, eval=FALSE, include=FALSE}
prior_mean <- 1.5
prior_count <- 1
numerator <- ifelse(!is.null(agent$reward_memory[[key]]), agent$reward_memory[[key]], 0) + prior_mean*prior_count
denominator <- ifelse(!is.null(agent$memory_count[[key]]), agent$memory_count[[key]], 0) + prior_count
avg <- numerator / denominator
```

##### Simulation

Simulation creates an plain of cells that all start from intact condition. Then agents start to operate in them, iteratively choosing actions and optimising rewards as described above. 
```{r simulations outpes, echo=TRUE}
simulate_adaptive <- function(steps = 100) {
  land_grid <- all_cells %>% mutate(rangeland = "intact") #assumes that all cells start from an intact condition = which is naive
  agent1 <- create_agent("A1")
  agent2 <- create_agent("A2")
  
  logs <- list()
  weather_trace <- character(steps)  # Store weather for debugging
  
  for (t in 1:steps) {
    weather <- draw_weather()
    weather_trace[t] <- weather
    cat(sprintf("Step %d — Weather: %s\n", t, weather))
    
    a1 <- select_action(agent1)
    a2 <- select_action(agent2)
    
    same_cell <- a1$x == a2$x && a1$y == a2$y
    
    a1_state <- land_grid %>% filter(x == a1$x, y == a1$y) %>% pull(rangeland)
    a2_state <- land_grid %>% filter(x == a2$x, y == a2$y) %>% pull(rangeland)
    
    a1_revisit <- isTRUE(agent1$last_cell["x"] == a1$x &&
                           agent1$last_cell["y"] == a1$y)
    a2_revisit <- isTRUE(agent2$last_cell["x"] == a2$x &&
                           agent2$last_cell["y"] == a2$y)
    
    r1 <- compute_reward(a1$action, same_cell, a1_revisit, a1_state)
    r2 <- compute_reward(a2$action, same_cell, a2_revisit, a2_state)
    
    new1 <- transition_rangeland(a1_state, a1$action, weather)
    new2 <- transition_rangeland(a2_state, a2$action, weather)
    
    land_grid[land_grid$x == a1$x & land_grid$y == a1$y, "rangeland"] <- new1
    land_grid[land_grid$x == a2$x & land_grid$y == a2$y, "rangeland"] <- new2
    
    #agent1 <- update_agent(agent1, a1, r1)
    #agent2 <- update_agent(agent2, a2, r2)
    
    logs[[t]] <- list(
      step = t,
      weather = weather,
      land_grid = land_grid,
      a1 = a1,
      a2 = a2
    )
  }
  
  return(list(logs = logs, weather_history = weather_trace))
}

```

```{r simulation result outpes, include=FALSE}
results <- simulate_adaptive(step=140)
```


##### Results

Ecosystem degradation rate over time
```{r data reshape outpes, echo=FALSE, warning=FALSE}
degradation_df <- map_dfr(results$logs, ~ .x$land_grid %>%
                            mutate(step = .x$step,
                                   degraded = rangeland == "degraded"))

degradation_summary <- degradation_df %>%
  group_by(step) %>%
  summarise(pct_degraded = mean(degraded) * 100)

ggplot(degradation_summary, aes(x = step, y = pct_degraded)) +
  geom_line(size = 1.2, color = "darkred") +
  labs(title = "Degradation Over Time", y = "% Degraded", x = "Step")
```


Policy mix by cell by each agent

```{r data reshape policy mix outpes, echo=FALSE, warning=FALSE}
action_df <- map2_dfr(results$logs, results$weather_history, function(res, weather) {
  tibble(
    step = res$step,
    weather = weather,
    x1 = res$a1$x,
    y1 = res$a1$y,
    a1 = res$a1$action,
    x2 = res$a2$x,
    y2 = res$a2$y,
    a2 = res$a2$action
  )
})
action_long <- action_df %>%
  pivot_longer(
    cols = c(x1, y1, a1, x2, y2, a2),
    names_to = c(".value", "agent"),
    names_pattern = "([a-z]+)([12])"
  ) %>%
  mutate(agent = paste0("A", agent))
action_long <- action_long %>% rename(action = a)

policy_map <- action_long %>%
  group_by(agent, x, y) %>%
  count(action) %>%
  slice_max(n, n = 1, with_ties = FALSE)

ggplot(policy_map, aes(x = x, y = y, fill = action)) +
  geom_tile(color = "white") +
  facet_wrap(~agent) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Dominant Stocking Rate by Agent", x = "land_grid X", y = "land_grid Y")
```

